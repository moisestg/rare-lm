\begin{abstract}
	
Recurrent neural network language models (RNNLM) have become a key element for state-of-the-art language modeling and are integral parts in a wide range of applications such as speech recognition or information retrieval systems. However, recent attempts have revealed the lack of genuine understanding of standard language models. In particular, the LAMBADA dataset consists of a collection of hard word prediction examples that require to keep track of information in the broader discourse. 

In this thesis we delve into the nature of LAMBADA and highlight the impact that rare words have on the overall results. We introduce our proposed model, the Softmax Mixture, and showcase through a series of experiments its ability to handle rare words when compared to vanilla RNNLMs. We further implement the Pointer Sentinel Mixture model and demonstrate that it achieves perplexities on par with the state-of-the-art on LAMBADA while having superior adaptation capabilities.

\noindent\textbf{Keywords:} language modeling, recurrent neural networks (RNN), LAMBADA dataset, rare word problem, pointer models.
 
\end{abstract}
