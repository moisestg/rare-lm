\chapter{Related Work}

This chapter gives a brief overview of the state-of-the-art techniques used for word-level neural language modeling.

Recurrent neural network language models (RNNLM) \cite{mikolov2010recurrent} introduced by Mikolov et al. (2010) have become ubiquitous in modern language modeling. RNNLMs incorporate the advantages of previous attempts at neural language modeling \cite{bengio2003neural}, namely projecting words into low dimensional space solving smoothing implicitly and being trained with standard backpropagation techniques. In addition to that, recurrent networks have the ability to learn how to compress an arbitrary long history into low dimensional space. Jozefowicz et al. (2016) explored the performance gains achieved by these architectures when trained on a large scale dataset \cite{jozefowicz2016exploring}.

Several improvements have since then been proposed to improve on vanilla RNNLMs:

Kim et al. (2015) introduced a novel method that only relies on character-level information to predict words \cite{kim2016character}. Word characters are processed by a one-dimensional convolutional layer and fed through a highway network in order to obtain the word representation. Despite requiring fewer parameters, character-aware models are on par with their word-level counterparts.

A challenge when using large vocabularies is the fact that calculating the output probability of a word implies computing the whole distribution due to the normalization necessary for the softmax operation. Mnih \& Hinton (2009) employ a hierarchical representation of the vocabulary in the form of a binary tree in which the output probability of a word is directly encoded in the path specified by the hierarchy \cite{mnih2009scalable}. Another approach relies on Noise Contrastive Estimation (NCE), which introduces a surrogate binary classification task in which a classifier is trained to discriminate between true data and samples coming from a noise distribution \cite{mnih2012fast}.

Regarding regularization, Zaremba et al. (2014) was one of the first to successfully apply dropout to RNNs \cite{zaremba2014recurrent}. A random binary mask is only applied to the non-recurrent connections, the cell's inputs and the outputs. In \cite{gal2016theoretically}, Gal \& Ghahramani (2016) provide a bayesian interpretation for dropout and argue that the mask should be the same for all time steps and also applied to the recurrent connections. A different approach is taken by Krueger et al. (2016) in \cite{krueger2016zoneout}, where randomly selected neurons of the hidden state are not updated (in contrast with being dropped). More recently, Merity et al. (2017) introduced another regularization variant where dropout is applied to the hidden-to-hidden recurrent weights rather than directly to the hidden states. In combination with other techniques, this method has become the state-of-the-art on several datasets \cite{merity2017regularizing}.

Following the work in deep RNNs, recurrent highway networks (RHN) \cite{zilly2016recurrent} introduced by Zilly et al. (2016) extend LSTMs to enable the learning of deep recurrent state transitions. This is achieved by modifying the LSTM cell to allow multiple hidden state updates per time step.

Inan et al. (2016) make a case for sharing the weights between the embeddings and the softmax layer \cite{inan2016tying}, which improves previous results and significantly reduces the amount of trainable parameters. The technique is theoretically motivated and takes advantage of the metric encoded into the space of word embeddings to generate a more informed target distribution. This prevents the model from having to learn a one-to-one correspondence between the input and output.

Similar to other areas of NLP, several memory augmented architectures have been proposed for language modeling. Daniluk et al. (2017) formulate an attention mechanism for RNNLMs \cite{daniluk2017frustratingly} and explores to what extent is the learning of long-range dependencies improved. Grave et al. (2016) and Merity et. al (2016) propose pointer based attention models where a probability distribution is generated over the history of recent words and blended with the distribution over the whole vocabulary. While the former doesn't need to be trained \cite{grave2016improving}, the latter introduces additional parametrization that allows to calculate the interpolation weight of the two distributions dynamically \cite{merity2016pointer}.

A very interesting research direction has been recently proposed by Zoph \& Le (2016), where a reinforcement learning agent is used to generate a custom
RNN cell architectures tailored to the specific task of language modeling \cite{zoph2016neural}.

With respect to the evaluation of RNNLMs, Melis et al. (2017) stress the importance of careful hyperparemeter tuning and arrive at the conclussion that standard LSTM architectures, when properly regularised, outperform more recent models \cite{melis2017state}. Paperno et al. (2016) raise their concerns with regards to the genuine language understanding of SOTA models and introduce LAMBADA \cite{paperno2016lambada}, a dataset specifically designed to put this to the test. Along these lines, Jia \& Liang (2017) propose a method to build adversarial examples and find out that when evaluated on them, models produce very poor results under standard evaluation metrics \cite{jia2017adversarial}.

\begin{table}[H]
	\centering
	\begin{tabular}{c|c|c|}
		\cline{2-3}
		& \textbf{Local} & \textbf{Global} \\ \hline
		\multicolumn{1}{|c|}{\textbf{Domain}}     & Trivial to do  & us, Dger        \\ \hline
		\multicolumn{1}{|c|}{\textbf{Vocabulary}} & Socher         & hardest         \\ \hline
	\end{tabular}
	\caption{Model's classification}
	\label{classification}
\end{table}
