## Literature

* Datasets:

    * LAMBADA ([link](https://arxiv.org/pdf/1606.06031)):

    * Children's Book Test - CBT ([link](https://arxiv.org/pdf/1511.02301.pdf)): 

* LM Baselines:

    * "Exploring the limits of language modeling" ([link](https://arxiv.org/pdf/1602.02410)): character CNN and LSTM networks to achieve state-of-the-art in the 1B Word dataset.

    * "Recurrent Highway Networks" ([link](https://arxiv.org/pdf/1607.03474)): modified LSTM that allows for deeper training (analogous to highway layers in feedforward nets). State-of-the-art results in several datasets.

* Attention models:

    * "Neural machine translation by jointly learning to align and translate" ([link](https://arxiv.org/pdf/1409.0473)): seminal paper for the application of attention models to a NLP task. Bidirectional encoder, alignment function for attention.

    * "Show, attend and tell: Neural image caption generation with visual attention" ([link](http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf)): hard and soft attention.

    * "Effective approaches to attention-based neural machine translation" ([link](https://arxiv.org/pdf/1508.04025)): global and local attention.

    * "Frustratingly Short Attention Spans in Neural Language Modeling" ([link](https://arxiv.org/pdf/1702.04521)): attention applied to LM. “Key-Value-Predict” attention (explicit split of the hidden state among the different tasks). Conclusions only made for the CBT dataset.

    * "Reasoning about entailment with neural attention" ([link](https://arxiv.org/pdf/1509.06664)): conditional encoding through attention.

    * "Teaching machines to read and comprehend" ([link](http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf)): attention applied to read comprehension tasks.

    * "Pointer Sentinel Mixture Models" ([link](https://arxiv.org/pdf/1609.07843.pdf)): similar to the continuous cache approach? Combination of selection of previous input words (pointer) and the base LSTM recurrent language model.

* Memory networks:

    * "Memory Networks" ([link](https://arxiv.org/pdf/1410.3916)): seminal paper on Memory Networks. 

    * "End-to-end memory networks" ([link](http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf)): modification of the original architecture to make it fully differentiable. 

* Hierarchical models:

    * "A clockwork RNN" ([link](https://arxiv.org/pdf/1402.3511)): 

    * "Hierarchical multiscale recurrent neural networks" ([link](https://arxiv.org/pdf/1609.01704)):

* Latent variables:

    * "A Latent Variable Recurrent Neural Network for Discourse Relation Language Models" ([link](https://arxiv.org/pdf/1603.01913)): introduces latent variables to model discourse relations among sentences. By marginalizing over them, we obtain a discourse informed LM.

    * "Improving Distributed Word Representation and Topic Model by Word-Topic Mixture Model" ([link](http://www.jmlr.org/proceedings/papers/v63/Fu60.pdf)): trains word embeddings and topic model simultaneously based on LDA and word embeddings to obtain better word representations. Gibbs sampling, limited use.

* Recent papers on LAMBADA:

    * "Improving Neural Language Models with a Continuous Cache" ([link](https://arxiv.org/pdf/1612.04426)): memory augmented RNN that stores previous words together with its respective hidden state generated by the RNN. Elements in the memory are accessed by a cross-product of the current hidden state and the ones in the cache, so it doesn’t need training and can be applied on top of an already trained model.

    * "Linguistic Knowledge as Memory for Recurrent Neural Networks" ([link](https://arxiv.org/pdf/1703.02620)): apply preprocessing to text (coreference, entity extraction,...) and use to guide the attention mechanism. State-of-the-art results on LAMBADA?

