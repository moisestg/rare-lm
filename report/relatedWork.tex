\chapter{Related Work}

This chapter gives a brief overview of the state-of-the-art techniques used for word-level neural language modeling.

Recurrent neural network language models (RNNLM) \cite{mikolov2010recurrent} introduced by Mikolov et al. (2010) have become ubiquitous in modern language models. RNNLMs incorporate the advantages of previous attempts at neural language modeling \cite{bengio2003neural}: Namely projecting words into low dimensional space solving smoothing implicitly and being trained with standard backpropagation techniques. In addition to that, recurrent networks have the ability to learn how to compress arbitrary long histories into low dimensional space. Jozefowicz et al. (2016) reported the performance gains achieved by these architectures, outperforming competing models when trained on large scale datasets \cite{jozefowicz2016exploring}.

Since then, several improvements have been proposed to improve on vanilla RNNLMs:

Kim et al. (2015) introduced a novel method that only relies on character-level information while still making predictions on the word level \cite{kim2016character}. Word characters are processed by a one-dimensional convolutional layer and fed through a highway network in order to obtain the word representations. Among their advantages, character-aware models require fewer parameters by removing the need for trainable word embeddings and improve over their word-level counterparts when applied to morphologically rich languages.

On a different note, using large vocabularies can be sometimes unfeasible as calculating the output probability of a word implies computing the whole distribution due to the normalization necessary for the softmax operation, which makes training prohibitively slow. Mnih \& Hinton (2009) employed a hierarchical representation of the vocabulary in the form of a binary tree in which the output probability of a word is directly encoded in the path specified by the hierarchy \cite{mnih2009scalable}. Another possible approach relies on Noise Contrastive Estimation (NCE), which introduces a surrogate binary classification task in which a classifier is trained to discriminate between the true data and samples coming from a noise distribution \cite{mnih2012fast}. In both cases normalization is rendered unnecessary, which makes this techniques appealing for large scale models.

Regarding regularization, Zaremba et al. (2014) were one of the first to successfully apply dropout to RNNs \cite{zaremba2014recurrent}. A random binary mask is only applied to the non-recurrent connections, the cell's inputs and outputs. In \cite{gal2016theoretically}, Gal \& Ghahramani (2016) provided a bayesian interpretation for dropout (known as variational dropout) and use it to argue that the mask should be the same for all time steps and also applied to the recurrent connections. A different approach called zoneout was taken by Krueger et al. (2016) in \cite{krueger2016zoneout}, where randomly selected neurons of the hidden state are not updated (in contrast with being dropped). More recently Merity et al. (2017) introduced another regularization variant where dropout is applied to the hidden-to-hidden recurrent weights rather than directly to the hidden states. In combination with other techniques, this method has achieved state-of-the-art perplexities on several datasets \cite{merity2017regularizing}.

Following the work in deep RNNs, recurrent highway networks (RHN) \cite{zilly2016recurrent} introduced by Zilly et al. (2016) extend LSTMs to enable the learning of deep recurrent state transitions. This is achieved by modifying the LSTM cell to allow multiple hidden state updates per time step.

Inan et al. (2016) made a case for sharing the weights between the embeddings and the softmax layer \cite{inan2016tying}, which improves previous results and significantly reduces the amount of trainable parameters. This technique is theoretically motivated and takes advantage of the metric encoded into the space of word embeddings to generate a more informed target distribution. This prevents the model from having to learn a one-to-one correspondence between the input and output.

Similar to other areas of NLP, several memory augmented architectures have been proposed for language modeling. Daniluk et al. (2017) formulated an attention mechanism for RNNLMs \cite{daniluk2017frustratingly} and explored to what extent is the learning of long-range dependencies improved. Grave et al. (2016) and Merity et. al (2016) proposed pointer based attention models where a probability distribution is generated over the history of recent words and blended with the distribution over the whole vocabulary. While the former does not need to be trained \cite{grave2016improving}, the latter introduces additional parametrization that allows to calculate the interpolation weight of the two distributions dynamically \cite{merity2016pointer}.

A very interesting research direction has been recently proposed by Zoph \& Le (2016), where a reinforcement learning agent is used to generate custom
RNN cell architectures tailored to the specific task of language modeling \cite{zoph2016neural}. This technique allows to find good neural network architectures automatically without the need of explicit human intervention.

With respect to the evaluation of RNNLMs, Melis et al. (2017) stressed the importance of careful hyperparemeter tuning and arrived at the conclusion that standard LSTM architectures, when properly regularized, outperform more recent models \cite{melis2017state}. Paperno et al. (2016) raised their concerns with regards to the genuine language understanding of SOTA models and introduce LAMBADA \cite{paperno2016lambada}, a dataset specifically designed to put this to the test. As this thesis is focused on LAMBADA, \autoref{sec:lambada} studies its characteristics in detail. Along these lines, Jia \& Liang (2017) proposed a method to build adversarial examples for reading comprehension systems and found out that when evaluated on them, models produce very poor results under standard evaluation metrics \cite{jia2017adversarial}.