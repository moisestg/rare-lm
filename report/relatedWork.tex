\chapter{Related Work}

This chapter gives a brief overview of the state-of-the-art techniques used for word-level neural language modeling.

Recurrent neural network language models (RNNLM) \cite{mikolov2010recurrent} introduced by Mikolov et al. (2010) have become ubiquitous in modern language modeling. RNNLMs incorporate the advantages of previous attempts at neural language modeling \cite{bengio2003neural}, namely projecting words into low dimensional space solving smoothing implicitly and being trained with standard backpropagation techniques. In addition to that, recurrent networks have the ability to learn how to compress arbitrary long histories into low dimensional space. Jozefowicz et al. (2016) explored the performance gains achieved by these architectures when trained on large scale datasets \cite{jozefowicz2016exploring}.

Several improvements have since then been proposed to improve on vanilla RNNLMs:

Kim et al. (2015) introduced a novel method that only relies on character-level information to predict words, which is specially helpful for morphologically rich languages \cite{kim2016character}. Word characters are processed by a one-dimensional convolutional layer and fed through a highway network in order to obtain the word representations. Despite requiring fewer parameters, character-aware models are on par with their word-level counterparts.

On a different note, using large vocabularies can be sometimes unfeasible as calculating the output probability of a word implies computing the whole distribution due to the normalization necessary for the softmax operation, which makes training prohibitively slow. Mnih \& Hinton (2009) employ a hierarchical representation of the vocabulary in the form of a binary tree in which the output probability of a word is directly encoded in the path specified by the hierarchy \cite{mnih2009scalable}. Another possible approach relies on Noise Contrastive Estimation (NCE), which introduces a surrogate binary classification task in which a classifier is trained to discriminate between the true data and samples coming from a noise distribution \cite{mnih2012fast}. In both cases normalization is rendered unnecessary, which makes this techniques specially appealing for large scale models.

Regarding regularization, Zaremba et al. (2014) was one of the first to successfully apply dropout to RNNs \cite{zaremba2014recurrent}. A random binary mask is only applied to the non-recurrent connections, the cell's inputs and outputs. In \cite{gal2016theoretically}, Gal \& Ghahramani (2016) provide a bayesian interpretation for dropout (known as variational dropout) and use it to argue that the mask should be the same for all time steps and also applied to the recurrent connections. A different approach called zoneout is taken by Krueger et al. (2016) in \cite{krueger2016zoneout}, where randomly selected neurons of the hidden state are not updated (in contrast with being dropped). More recently Merity et al. (2017) introduced another regularization variant where dropout is applied to the hidden-to-hidden recurrent weights rather than directly to the hidden states. In combination with other techniques, this method has achieved state-of-the-art perplexities on several datasets \cite{merity2017regularizing}.

Following the work in deep RNNs, recurrent highway networks (RHN) \cite{zilly2016recurrent} introduced by Zilly et al. (2016) extend LSTMs to enable the learning of deep recurrent state transitions. This is achieved by modifying the LSTM cell to allow multiple hidden state updates per time step.

Inan et al. (2016) make a case for sharing the weights between the embeddings and the softmax layer \cite{inan2016tying}, which improves previous results and significantly reduces the amount of trainable parameters. This technique is theoretically motivated and takes advantage of the metric encoded into the space of word embeddings to generate a more informed target distribution. This prevents the model from having to learn a one-to-one correspondence between the input and output.

Similar to other areas of NLP, several memory augmented architectures have been proposed for language modeling. Daniluk et al. (2017) formulate an attention mechanism for RNNLMs \cite{daniluk2017frustratingly} and explores to what extent is the learning of long-range dependencies improved. Grave et al. (2016) and Merity et. al (2016) proposed pointer based attention models where a probability distribution is generated over the history of recent words and blended with the distribution over the whole vocabulary. While the former doesn't need to be trained \cite{grave2016improving}, the latter introduces additional parametrization that allows to calculate the interpolation weight of the two distributions dynamically \cite{merity2016pointer}.

A very interesting research direction has been recently proposed by Zoph \& Le (2016), where a reinforcement learning agent is used to generate custom
RNN cell architectures tailored to the specific task of language modeling \cite{zoph2016neural}.

With respect to the evaluation of RNNLMs, Melis et al. (2017) stress the importance of careful hyperparemeter tuning and arrive at the conclussion that standard LSTM architectures, when properly regularised, outperform more recent models \cite{melis2017state}. Paperno et al. (2016) raise their concerns with regards to the genuine language understanding of SOTA models and introduce LAMBADA \cite{paperno2016lambada}, a dataset specifically designed to put this to the test. Along these lines, Jia \& Liang (2017) propose a method to build adversarial examples for reading comprehension systems and find out that when evaluated on them, models produce very poor results under standard evaluation metrics \cite{jia2017adversarial}.